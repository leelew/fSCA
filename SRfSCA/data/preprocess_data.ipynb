{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "client = Client(n_workers=10, threads_per_worker=5, memory_limit='6GB')\n",
    "\n",
    "# aggregate 500m HMASR to 10km resolution\n",
    "def agg_HMASR(year, path, out_path):\n",
    "    \"\"\"aggregate the HMASR data to 10km resolution\"\"\"\n",
    "    # parameters\n",
    "    path = path+str(year)+'/'\n",
    "    out_path = out_path+str(year)+'/'\n",
    "    ds_name_list = ['FORCING_POST', 'SWE_SCA_POST', 'SD_POST']\n",
    "\n",
    "    for ds_name in ds_name_list:\n",
    "        # list all files\n",
    "        list_files = [f for f in os.listdir(path) if ds_name in f] \n",
    "\n",
    "        ds_10km_mask = [] # apply non-seasonal snow mask\n",
    "        for file in list_files:\n",
    "            ds = delayed(xr.open_dataset)(path+file)\n",
    "            mask = delayed(xr.open_dataset)(path+file.replace(ds_name, 'MASK'))\n",
    "            if ds_name in ['FORCING_POST']:\n",
    "                ds_10km_mask.append(ds.where(mask.Non_seasonal_snow_mask == 0).coarsen(Latitude=25, Longitude=25).mean())\n",
    "            else:\n",
    "                # Select only the mean (Stats=0)\n",
    "                ds_10km_mask.append(ds.isel(Stats=0).where(mask.Non_seasonal_snow_mask == 0).coarsen(Latitude=25, Longitude=25).mean())\n",
    "        ds_10km_mask = compute(ds_10km_mask)\n",
    "        xr.save_mfdataset(ds_10km_mask[0], [out_path+file.replace(ds_name, ds_name+'_MASK') for file in list_files])\n",
    "\n",
    "for i in range(2002, 2003):\n",
    "    agg_HMASR(i, '/tera04/lilu/HMASR/', '/tera06/lilu/fSCA/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the 10km resolution data to a single file\n",
    "def combine_mfdata(year, path, out_path):\n",
    "    ds_name_list = ['FORCING_POST', 'SWE_SCA_POST', 'SD_POST']\n",
    "\n",
    "    for ds_name in ds_name_list:\n",
    "        print(ds_name)\n",
    "        ds = xr.open_mfdataset(path+str(year)+'/*'+str(year)+'*'+ds_name+'_MASK.nc')\n",
    "        ds = ds.assign_coords(Day=pd.date_range(start='{year}-10-01'.format(year=year), periods=ds.Day.size, freq='D'))\n",
    "        ds = ds.rename({'Longitude': 'lon', 'Latitude': 'lat', 'Day': 'time'}).transpose(\"time\", \"lat\", \"lon\")\n",
    "        if ds_name in ['FORCING_POST']:\n",
    "            ds = xr.where(ds.Ta_Post>30, ds, np.nan)\n",
    "            ds = ds.reindex(lat=list(reversed(ds.lat)))\n",
    "        ds.to_netcdf(out_path+'HMA_SR_D_v01_10km_'+str(year)+'_'+ds_name+'_MASK.nc')\n",
    "\n",
    "for i in range(2002, 2003):\n",
    "    combine_mfdata(i, '/tera06/lilu/fSCA/data/', '/tera06/lilu/fSCA/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 90m for the MERITHydro dataset\n",
    "lat = np.zeros((6000*4,))*np.nan\n",
    "for i, nlat in enumerate(range(40, 24, -5)):\n",
    "    filename = os.path.join('/tera02/zhangsp/tera07/MERITHydro/n{lat:02}e095.nc'.format(lat=nlat))\n",
    "    f = xr.open_dataset(filename)\n",
    "    lat[6000*i:6000*(i+1)] = np.array(f['latitude'][:])\n",
    "\n",
    "lon = np.zeros((6000*9,))*np.nan\n",
    "for j, nlon in enumerate(range(60, 105, 5)):\n",
    "    filename = os.path.join('/tera02/zhangsp/tera07/MERITHydro/n25e{lon:03}.nc'.format(lon=nlon))\n",
    "    f = xr.open_dataset(filename)\n",
    "    lon[6000*j:6000*(j+1)] = np.array(f['longitude'][:])\n",
    "\n",
    "# 25-45N, 60-105E\n",
    "elv = np.zeros((6000*9, 6000*4))*np.nan\n",
    "for i, nlat in enumerate(range(40, 24, -5)):\n",
    "    for j, nlon in enumerate(range(60, 105, 5)):\n",
    "        print(nlat, nlon)\n",
    "        filename = os.path.join('/tera02/zhangsp/tera07/MERITHydro/n{lat:02}e{lon:03}.nc'.format(lat=nlat, lon=nlon))\n",
    "        f = xr.open_dataset(filename)\n",
    "        elv[6000*j:6000*(j+1), 6000*i:6000*(i+1)] = np.array(f['elv'][:])\n",
    "\n",
    "# save\n",
    "f = nc.Dataset('MERITHydro_elv_90m_TP.nc', 'w', format='NETCDF4')\n",
    "f.createDimension('longitude', size=lon.shape[0])\n",
    "f.createDimension('latitude', size=lat.shape[0])\n",
    "\n",
    "lon0 = f.createVariable('longitude', 'f4', dimensions='longitude')\n",
    "lon0.units = \"degrees_east\"\n",
    "lon0.long_name = \"longitude\"\n",
    "lon0.standard_name = \"longitude\"\n",
    "lon0.axis = \"X\"\n",
    "\n",
    "lat0 = f.createVariable('latitude', 'f4', dimensions='latitude')\n",
    "lat0.units = \"degrees_north\"\n",
    "lat0.axis = \"Y\"\n",
    "lat0.long_name = \"latitude\"\n",
    "lat0.standard_name = \"latitude\"\n",
    "\n",
    "data = f.createVariable('elv', 'f4', dimensions=('longitude','latitude'))\n",
    "data.units = \"m\"\n",
    "lon0[:], lat0[:], data[:] = lon, lat, elv\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the subgrid standard deviation for the MERITHydro dataset\n",
    "f = xr.open_dataset('MERITHydro_elv_90m_TP.nc')\n",
    "elv_90m = np.array(f['elv'][:])\n",
    "lat_90m, lon_90m = np.array(f['latitude'][:]), np.array(f['longitude'][:])\n",
    "f = xr.open_dataset('MERITHydro_elv_0p1_TP.nc')\n",
    "lat_0p1, lon_0p1 = np.array(f['lat'][:]), np.array(f['lon'][:])\n",
    "\n",
    "# calculate the standard deviation\n",
    "topo_std = np.zeros((len(lon_0p1), len(lat_0p1)))*np.nan\n",
    "for i in range(len(lat_0p1)):\n",
    "    for j in range(len(lon_0p1)):\n",
    "        print(i, j)\n",
    "        lat0, lon0 = lat_0p1[i], lon_0p1[j]\n",
    "\n",
    "        # identify coordinates of select 500m grid\n",
    "        if i == 0:\n",
    "            lat1 = lat_0p1[i+1]\n",
    "            dlat = np.abs((lat1-lat0)/2)\n",
    "            blat, ulat = lat0-dlat, lat0+dlat\n",
    "        elif i == len(lat_0p1)-1:\n",
    "            lat1 = lat_0p1[i-1]\n",
    "            dlat = np.abs((lat1-lat0)/2)\n",
    "            blat, ulat = lat0-dlat, lat0+dlat\n",
    "        else:\n",
    "            lat1, lat2 = lat_0p1[i-1], lat_0p1[i+1]\n",
    "            dlat1, dlat2 = np.abs((lat1-lat0)/2), np.abs((lat2-lat0)/2)\n",
    "            blat, ulat = lat0-dlat1, lat0+dlat2\n",
    "\n",
    "        if j == 0:\n",
    "            lon1 = lon_0p1[j+1]\n",
    "            dlon = np.abs((lon1-lon0)/2)\n",
    "            llon, rlon = lon0-dlon, lon0+dlon\n",
    "        elif j == len(lon_0p1)-1:\n",
    "            lon1 = lon_0p1[j-1]\n",
    "            dlon = np.abs((lon1-lon0)/2)\n",
    "            llon, rlon = lon0-dlon, lon0+dlon\n",
    "        else:  \n",
    "            lon1, lon2 = lon_0p1[j-1], lon_0p1[j+1]\n",
    "            dlon1, dlon2 = np.abs((lon1-lon0)/2), np.abs((lon2-lon0)/2)\n",
    "            llon, rlon = lon0-dlon1, lon0+dlon2\n",
    "\n",
    "        # find 90m grids locate in this grid\n",
    "        idx_lat = np.where((lat_90m>=blat) & (lat_90m<=ulat))[0]\n",
    "        idx_lon = np.where((lon_90m>=llon) & (lon_90m<=rlon))[0]\n",
    "        tmp_elv = elv_90m[idx_lon,:][:,idx_lat]\n",
    "        print(tmp_elv.shape)\n",
    "        # calculate std\n",
    "        topo_std[j, i] = np.nanstd(tmp_elv)\n",
    "\n",
    "# save\n",
    "f = nc.Dataset('MERITHydro_std_0p1_TP.nc', 'w', format='NETCDF4')\n",
    "f.createDimension('lon', size=lon_0p1.shape[0])\n",
    "f.createDimension('lat', size=lat_0p1.shape[0])\n",
    "\n",
    "lon0 = f.createVariable('lon', 'f4', dimensions='lon')\n",
    "lon0.units = \"degrees_east\"\n",
    "lon0.long_name = \"longitude\"\n",
    "lon0.standard_name = \"longitude\"\n",
    "lon0.axis = \"X\"\n",
    "\n",
    "lat0 = f.createVariable('lat', 'f4', dimensions='lat')\n",
    "lat0.units = \"degrees_north\"\n",
    "lat0.axis = \"Y\"\n",
    "lat0.long_name = \"latitude\"\n",
    "lat0.standard_name = \"latitude\"\n",
    "\n",
    "data = f.createVariable('elv_std', 'f4', dimensions=('lon','lat'))\n",
    "data.units = \"m\"\n",
    "lon0[:], lat0[:], data[:] = lon_0p1, lat_0p1, topo_std\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of permanent snow in each 0.1 grid\n",
    "ds = xr.open_mfdataset(\"/tera04/lilu/HMASR/1999/HMA_SR_D_v01_*_agg_16_WY1999_00_MASK.nc\")\n",
    "permanent_snow_count = ds.coarsen(Latitude=25, Longitude=25).sum().load()\n",
    "permanent_snow_count = permanent_snow_count.rename({'Longitude': 'lon', 'Latitude': 'lat', 'Water_Year': 'year'}).transpose(\"year\",\"lat\", \"lon\")\n",
    "mask_psnow = (permanent_snow_count/(25*25)*100)\n",
    "mask_psnow.Non_seasonal_snow_mask.rename('P_psnow').to_netcdf('ratio_psnow.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_lai_day(ds):\n",
    "    out = []\n",
    "    for i in range(2000, 2014):\n",
    "        out.append(ds.sel(time=slice('{year}-01-01'.format(year=i), '{year}-12-31'.format(year=i), 8)).values)\n",
    "    out = np.concatenate(out, axis=0)\n",
    "    return out\n",
    "\n",
    "ds = xr.open_dataset('SD.nc')\n",
    "sd_train = slice_lai_day(ds.SD_Post)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using all avaliable raw data (aware of dimensonless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4132162/2111100786.py:19: RuntimeWarning: divide by zero encountered in divide\n",
      "  rho_train = swe_train/sd_train\n",
      "/tmp/ipykernel_4132162/2111100786.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  rho_train = swe_train/sd_train\n",
      "/tmp/ipykernel_4132162/2111100786.py:20: RuntimeWarning: divide by zero encountered in divide\n",
      "  rho_test = swe_test/sd_test\n",
      "/tmp/ipykernel_4132162/2111100786.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  rho_test = swe_test/sd_test\n"
     ]
    }
   ],
   "source": [
    "# make train data\n",
    "# define train and test period\n",
    "train_period = slice('1999-10-01', '2013-09-30') # ~80%\n",
    "test_period = slice('2013-10-01', '2017-09-30') # ~20%\n",
    "\n",
    "# read snow depth\n",
    "ds = xr.open_dataset('SD.nc')\n",
    "sd_train = ds.SD_Post.sel(time=train_period).values\n",
    "sd_test = ds.SD_Post.sel(time=test_period).values\n",
    "nt_train, nt_test = sd_train.shape[0], sd_test.shape[0]\n",
    "lat, lon = ds.lat, ds.lon\n",
    "\n",
    "# read snow water equivalent and snow cover\n",
    "ds = xr.open_dataset('SWE_SCA.nc')\n",
    "swe_train = ds.SWE_Post.sel(time=train_period).values\n",
    "swe_test = ds.SWE_Post.sel(time=test_period).values\n",
    "sca_train = ds.SCA_Post.sel(time=train_period).values\n",
    "sca_test = ds.SCA_Post.sel(time=test_period).values\n",
    "rho_train = swe_train/sd_train\n",
    "rho_test = swe_test/sd_test\n",
    "rho_train[np.where((rho_train>1000) | (rho_train<0))] = np.nan\n",
    "rho_test[np.where((rho_test>1000) | (rho_test<0))] = np.nan\n",
    "\n",
    "# read topo-related vars\n",
    "ds = xr.open_dataset('MERITHydro/MERITHydro_0p1_TP.nc')\n",
    "elv_std = ds.elv_std.values[np.newaxis]   \n",
    "elv = ds.elv.values[np.newaxis]\n",
    "slp = ds.slp.values[np.newaxis]\n",
    "asp = ds.asp.values[np.newaxis]\n",
    "\n",
    "# read forcing\n",
    "ds = xr.open_dataset('FORCING.nc')\n",
    "ta_train = ds.Ta_Post.sel(time=train_period).values\n",
    "rs_train = ds.Rs_Post.sel(time=train_period).values\n",
    "rl_train = ds.Rl_Post.sel(time=train_period).values\n",
    "ps_train = ds.Ps_Post.sel(time=train_period).values\n",
    "ppt_train = ds.PPT_Post.sel(time=train_period).values\n",
    "q_train = ds.q_Post.sel(time=train_period).values\n",
    "\n",
    "ta_test = ds.Ta_Post.sel(time=test_period).values\n",
    "rs_test = ds.Rs_Post.sel(time=test_period).values\n",
    "rl_test = ds.Rl_Post.sel(time=test_period).values\n",
    "ps_test = ds.Ps_Post.sel(time=test_period).values\n",
    "ppt_test = ds.PPT_Post.sel(time=test_period).values\n",
    "q_test = ds.q_Post.sel(time=test_period).values\n",
    "\n",
    "x_train = np.stack([sd_train, swe_train, ta_train, rs_train, rl_train, ps_train, ppt_train, q_train, \\\n",
    "    np.tile(elv_std, (nt_train,1,1)), np.tile(np.sin(slp), (nt_train,1,1)), np.tile(np.cos(asp), (nt_train,1,1))], axis=-1).reshape(-1, 11)\n",
    "y_train = sca_train.reshape(-1,1)*100\n",
    "\n",
    "x_test = np.stack([sd_test, swe_test, ta_test, rs_test, rl_test, ps_test, ppt_test, q_test, \\\n",
    "    np.tile(elv_std, (nt_test,1,1)), np.tile(np.sin(slp), (nt_test,1,1)), np.tile(np.cos(asp), (nt_test,1,1))], axis=-1).reshape(-1, 11)\n",
    "y_test = sca_test.reshape(-1,1)*100\n",
    "\n",
    "y_train = np.delete(y_train, np.where(np.isnan(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isnan(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isnan(y_train))[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(np.isnan(y_train))[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(np.isinf(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isinf(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isinf(y_train))[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(np.isinf(y_train))[0], axis=0)\n",
    "\n",
    "y_test = np.delete(y_test, np.where(np.isnan(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isnan(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isnan(y_test))[0], axis=0)\n",
    "y_test = np.delete(y_test, np.where(np.isnan(y_test))[0], axis=0)\n",
    "y_test = np.delete(y_test, np.where(np.isinf(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isinf(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isinf(y_test))[0], axis=0)\n",
    "y_test = np.delete(y_test, np.where(np.isinf(y_test))[0], axis=0)\n",
    "\n",
    "np.save('x_train_a.npy', x_train)\n",
    "np.save('y_train_a.npy', y_train)\n",
    "np.save('x_test_a.npy', x_test)\n",
    "np.save('y_test_a.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using preprocesses dimensionless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4132162/3331777666.py:34: RuntimeWarning: divide by zero encountered in divide\n",
      "  rho_train = swe_train/sd_train\n",
      "/tmp/ipykernel_4132162/3331777666.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  rho_train = swe_train/sd_train\n",
      "/tmp/ipykernel_4132162/3331777666.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  rho_test = swe_test/sd_test\n",
      "/tmp/ipykernel_4132162/3331777666.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  rho_test = swe_test/sd_test\n",
      "/tmp/ipykernel_4132162/3331777666.py:39: RuntimeWarning: divide by zero encountered in divide\n",
      "  x_train = np.stack([sd_train/0.01, 1/rho_train, swe_train/swe_max, ta_train/273.15, q_train, \\\n",
      "/tmp/ipykernel_4132162/3331777666.py:40: RuntimeWarning: divide by zero encountered in divide\n",
      "  200/np.tile(elv_std, (nt_train,1,1))], axis=-1).reshape(-1, 6)\n",
      "/tmp/ipykernel_4132162/3331777666.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  x_test = np.stack([sd_test/0.01, 1/rho_test, swe_test/swe_max, ta_test/273.15, q_test, \\\n",
      "/tmp/ipykernel_4132162/3331777666.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  200/np.tile(elv_std, (nt_test,1,1))], axis=-1).reshape(-1, 6)\n"
     ]
    }
   ],
   "source": [
    "# make train data using designed factors\n",
    "# define train and test period\n",
    "train_period = slice('1999-10-01', '2013-09-30') # ~80%\n",
    "test_period = slice('2013-10-01', '2017-09-30') # ~20%\n",
    "\n",
    "# read snow depth\n",
    "ds = xr.open_dataset('SD.nc')\n",
    "sd_train = ds.SD_Post.sel(time=train_period).values\n",
    "sd_test = ds.SD_Post.sel(time=test_period).values\n",
    "nt_train, nt_test = sd_train.shape[0], sd_test.shape[0]\n",
    "lat, lon = ds.lat, ds.lon\n",
    "\n",
    "# read snow water equivalent and snow cover\n",
    "ds = xr.open_dataset('SWE_SCA.nc')\n",
    "swe_train = ds.SWE_Post.sel(time=train_period).values\n",
    "swe_test = ds.SWE_Post.sel(time=test_period).values\n",
    "sca_train = ds.SCA_Post.sel(time=train_period).values\n",
    "sca_test = ds.SCA_Post.sel(time=test_period).values\n",
    "swe, sca = ds.SWE_Post.values, ds.SCA_Post.values\n",
    "swe_max = np.nanmax(swe)\n",
    "\n",
    "# read topo-related vars\n",
    "ds = xr.open_dataset('MERITHydro/MERITHydro_0p1_TP.nc')\n",
    "elv_std = ds.elv_std.values[np.newaxis]   \n",
    "\n",
    "# read forcing\n",
    "ds = xr.open_dataset('FORCING.nc')\n",
    "ta_train = ds.Ta_Post.sel(time=train_period).values\n",
    "q_train = ds.q_Post.sel(time=train_period).values\n",
    "ta_test = ds.Ta_Post.sel(time=test_period).values\n",
    "q_test = ds.q_Post.sel(time=test_period).values\n",
    "\n",
    "# calculate own designed factors\n",
    "rho_train = swe_train/sd_train\n",
    "rho_test = swe_test/sd_test\n",
    "rho_train[np.where((rho_train>1000) | (rho_train<0))] = np.nan\n",
    "rho_test[np.where((rho_test>1000) | (rho_test<0))] = np.nan\n",
    "\n",
    "x_train = np.stack([sd_train/0.01, 1/rho_train, swe_train/swe_max, ta_train/273.15, q_train, \\\n",
    "    200/np.tile(elv_std, (nt_train,1,1))], axis=-1).reshape(-1, 6)\n",
    "y_train = sca_train.reshape(-1,1)*100\n",
    "\n",
    "x_test = np.stack([sd_test/0.01, 1/rho_test, swe_test/swe_max, ta_test/273.15, q_test, \\\n",
    "    200/np.tile(elv_std, (nt_test,1,1))], axis=-1).reshape(-1, 6)\n",
    "y_test = sca_test.reshape(-1,1)*100\n",
    "\n",
    "y_train = np.delete(y_train, np.where(np.isnan(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isnan(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isnan(y_train))[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(np.isnan(y_train))[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(np.isinf(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isinf(x_train))[0], axis=0)\n",
    "x_train = np.delete(x_train, np.where(np.isinf(y_train))[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(np.isinf(y_train))[0], axis=0)\n",
    "\n",
    "y_test = np.delete(y_test, np.where(np.isnan(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isnan(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isnan(y_test))[0], axis=0)\n",
    "y_test = np.delete(y_test, np.where(np.isnan(y_test))[0], axis=0)\n",
    "y_test = np.delete(y_test, np.where(np.isinf(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isinf(x_test))[0], axis=0)\n",
    "x_test = np.delete(x_test, np.where(np.isinf(y_test))[0], axis=0)\n",
    "y_test = np.delete(y_test, np.where(np.isinf(y_test))[0], axis=0)\n",
    "\n",
    "np.save('x_train_d.npy', x_train)\n",
    "np.save('y_train_d.npy', y_train)\n",
    "np.save('x_test_d.npy', x_test)\n",
    "np.save('y_test_d.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
